---
title: 'Choosing models - evaluating performance'
output:
  slidy_presentation:
    highlight: pygments
  html_document: default
  pdf_document: default
  ioslides_presentation:
    highlight: pygments
  beamer_presentation:
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(lubridate)
library(reldist)
library(purrr)
library(ggpubr)
```

# Background material

See materials in choosingmodels.pdf (Slide 1-22)

Some examples of model evaluation using results
from a hydrologic model applied to a Sierra watershed

# Graph

```{r simple}

sager = read.table("../Data/sager.txt", header=T)
head(sager)

# add date
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

# plot
sagerl = sager %>% pivot_longer(cols=c("model","obs"), names_to="source",
                                  values_to="flow")

# basic plot
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()

# change access to get a closer look at performance at low values
# when you have high dynamic range (lots of large and small values), taking log can help
# with visualization
ggplot(sagerl, aes(date, flow, col=source, linetype=source))+geom_line()+scale_y_continuous(trans="log")+labs(y="streamflow mm/day")

# look at it another way with 1:1 line 
ggplot(sager, aes(obs, model))+geom_point()+geom_abline(intercept=0, slope=1, col="red")


```

# Measure Performance using different metrics

- for the inputs to these functions, m=model and o=observation

```{r}


source("../R/nse.R")

source("../R/relerr.R")

source("../R/cper.R")

nse
relerr
cper

nse(m=sager$model, o=sager$obs)

relerr(m=sager$model, o=sager$obs)*100

cper(m=sager$model, o=sager$obs, weight.nse=0.8)

cor(sager$model, sager$obs)
```

# Scale and subsetting

Performance also depends on the 'what' you are evaluating

  * time steps (annual, daily, monthly)
  
  * selection of particular periods of time
  

  
```{r}
# try a different time step
sager_wy = sager %>% group_by(wy) %>% summarize(model=sum(model), obs=sum(obs))

nse(sager_wy$model, sager_wy$obs)
cper(m=sager_wy$model, o=sager_wy$obs, weight.nse=0.8)

# just look at august flow
# first sum by month
tmp = sager %>% group_by(wy, month) %>% summarize(model=sum(model), obs=sum(obs))

# now extract august
sager_aug = subset(tmp, month==8)
cor(sager_aug$model, sager_aug$obs)

```

# In Class Exercise

* Read in the *sager.txt* data

* Think of a new metric that might be interesting from a particular environmental context

* Code that metric as a function - and then apply it

```{r}

compute_extremes <- function(m=sager$model, o=sager$obs, wy=sager$wy, wts=c(0.5, 0.5)){
   flow = cbind.data.frame(m, o, wy) 
   wy_flow = flow %>% 
     group_by(wy) %>% 
     summarize(min_obs = min(o),
               min_mod = min(m),
               max_obs = max(o),
               max_mod = max(m))
   
   # 1 over mean error so higher values are better
   min_err = mean(wy_flow$min_mod - wy_flow$min_obs)
   max_err = mean(wy_flow$max_mod - wy_flow$max_obs)
   
   min_err_trans = 1 - min(1.0, abs(min_err))
   max_err_trans = 1 - min(1.0, abs(max_err))
   
   combined = wts[1]*min_err_trans + wts[2]*max_err_trans
   
   
   return(list(min_err_trans, max_err_trans, combined))
  
}

 compute_extremes()
 
```


# Using multiple metrics. 

* depends on what you want the model to get right

* type of data that you have for evaluation (its resolution and accuracy)


```{r multimetric}

# turn your evaluation metric into a function
source("../R/compute_lowflowmetrics.R")
compute_lowflowmetrics

compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy)

# use different low flow months
compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, low_flow_months = c(7:9))

```
  
# Combining  multiple metrics into a single value 


* if you want a quantitative comparison between multiple models

* useful for calibration

**Slides 23-26**

```{r combined}

perf = compute_lowflowmetrics(m=sager$model,o=sager$obs, month=sager$month, day=sager$day, year=sager$year, wy=sager$wy, low_flow_months = c(7:9))

perf = as.data.frame((perf))

# remember you want error to be low but correlation to be high 
# so we need to transform in some way

# normalize by max error = if error is greater than this we don't care
# can try many ideas -  maybe 50% of mean daily summer observed low flow
tmp = sager %>% subset(month %in% c(7:9)) 
errmax = mean(tmp$obs)*0.5
errmax

perf = perf %>% mutate(annual_min_err_trans = max(0,(1-abs(annual_min_err/errmax) )))
      
# for monthly we can do a similar thing to find maximum allowable error   
tmp = sager %>% subset(month %in% c(7:9)) %>% group_by(wy, month) %>% summarize(obs=sum(obs))

errmax = mean(tmp$obs)*0.5
 
perf = perf %>% mutate(low_month_err_trans = max(0,(1-abs(low_month_err/errmax) )))

# now we have 4 measures that we can combine together

perf = perf %>% mutate(combined = (annual_min_cor + annual_min_err_trans + low_month_err_trans + low_month_cor)/4)
perf

# or weight differently - we know that minimum flows are hard to get so we can weight those differently

perf = perf %>% mutate(combined2 = 0.1*annual_min_cor + 0.1*annual_min_err_trans + 0.4*low_month_err_trans+ 0.4*low_month_cor)

perf

# easier to put all this in a function
source("../R/compute_lowflowmetrics_all.R")

compute_lowflowmetrics_all

```
# Your turn! Part 1: Come up with a combined metric that you think is interesting 

* include at least one "sub" metric that needs to be transformed (for example, the `annual_min_err_trans` above)

* Be creative 
    * you can subset, aggregate, focus only on particular type of years or days
    * think about ecological or human water uses that depend on certain flow conditions
    
```{r your_turn_part1}
# add your code here (or start new file if you want)

```

# Calibration

Calibration is picking parameter sets based on performance evaluation

Apply metrics over multiple outputs (generated by running across many parameters sets) - like we've done in our sensitivity analysis work

**Example** - a dataset where each column
is a different model run for Sagehen Creek
(using different parameters) - don't worry about the parameters for now

* sagerm.txt

**Split-sample**: split time period into 
  * calibration time period (used to pick parameter sets)
  * validation time period (used to see how well chose paramter sets perform)
  
  
**Thoughts**

In many cases - you just run calibration sample first - and then only run validation for parameters that you choose
here I ran for all parameters sets for the full time period so that we can explore

We could also envision this as a 'lab' where we only had a few years of observed streamflow data for calibration
and want to see going forward how much parameter selection influences results


Some code to help organize things

```{r multiple}

# multiple results - lets say we've run the model for multiple years, 
#each column  is streamflow for a different parameter set
msage = read.table("../Data/sagerm.txt", header=T)

# keep track of number of simulations (e.g results for each parameter set) 
# use as a column names
nsim = ncol(msage)
snames = sprintf("S%d",seq(from=1, to=nsim))
colnames(msage)=snames


# lets say we know the start date from our earlier output
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# lets add observed
msage = left_join(msage, sager[,c("obs","date")], by=c("date"))

head(msage)

# how can we plot all results - lets plot water year 1970 otherwise its hard to see
msagel = msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")

p1=ggplot(subset(msagel, wy == 1970), aes(as.Date(date), flow, col=run))+geom_line()+theme(legend.position = "none")
p1

# lets add observed streamflow
p1+geom_line(data=subset(sager, wy == 1970), aes(as.Date(date), obs), size=2, col="black", linetype=2)+labs(y="Streamflow", x="Date")

# subset for split sample calibration
short_msage = subset(msage, wy < 1975)

# compute performance measures for output from all parameters
res = short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      map_dbl(nse, short_msage$obs) # purrr function here! map_dbl will apply the function nse() to each column in our data frame against the observed and returns a vector

head(res)


# another example using our low flow statistics
# use apply to compute for all the data
source("../R/compute_lowflowmetrics_all.R")
res = short_msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>%
  map_df(compute_lowflowmetrics_all, o=short_msage$obs, month=short_msage$month, day=short_msage$day, year=short_msage$year, wy=short_msage$wy)
# note here we use map_df to get a dataframe back 


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)
# we can add a row that links with simulation number
res$sim = snames

# graph range of performance measures
resl = res %>% pivot_longer(-sim, names_to="metric", values_to="value")

ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")


# select the best one based on the combined metric
best = res[which.max(res$combined),]

# running the model forward
# so we can look at the full time series

# lets start with streamflow estimates from best performing parameter set
 ggplot(msage, aes(date, msage[,best$sim])) + geom_line()+geom_line(aes(date, obs), col="red") 

 
# for comparison lets consider how worst and best parameters perform for subsequent simulations
# focusing specifically on August streamflow
 worst = res[which.min(res$combined),]
 
 compruns = msage %>% select(best$sim, worst$sim, date, obs, month, day, year, wy)
 compruns = subset(compruns, wy > 1970)
 compruns_mwy = compruns %>% select(-c(day,date, year)) %>% group_by(month, wy) %>% summarize(across(everything(), mean))
 
 compruns_mwyl = compruns_mwy %>% pivot_longer(cols=!c(month,wy), names_to="sim", values_to="flow")
 compruns_mwyl %>% subset(month==8) %>% ggplot(aes(sim,flow ))+geom_boxplot()
 




```

# Your turn! Part 2: Using your performance metric

* Perform a split-sample calibration - you can decide what year to use for
calibration (its an experiment!)

* Find the best parameter set, and then graph something about streamflow (e.g daily, mean August, or ?) for the best parameter set

* Compute how the performance of the model using the best parameter set changed
in pre and post calibration periods

On the canvas survey - add the 'best' parameter set number (so we can compare how different metrics influence which parameter you pick)

```{r your_turn_part2}
# add your code here (or start new file if you want)
# subset for split sample calibration
short_msage = subset(msage, wy < 1975)

source("../R/compute_extremes.R")
res = short_msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>%
  map_df(compute_lowflowmetrics_all, o=short_msage$obs, month=short_msage$month, day=short_msage$day, year=short_msage$year, wy=short_msage$wy)
# note here we use map_df to get a dataframe back 


# interesting to look at range of metrics - could use this to decide on
# acceptable values
summary(res)
# we can add a row that links with simulation number
res$sim = snames

# graph range of performance measures
resl = res %>% pivot_longer(-sim, names_to="metric", values_to="value")

ggplot(resl, aes(metric, value))+geom_boxplot()+facet_wrap(~metric, scales="free")


# select the best one based on the combined metric
best = res[which.max(res$combined),]


```

# More on calibration - some complications


**slides 27-41

How to do a maximum likelihood estimate of model results in R

# Glue - generalized uncertainty analysis

What if we wanted to keep all of the 'good' parameters

* we could just keep them all as equally likely
* we could weight them by performance

Either way we can graph and come up with 'best' prediction accounting for uncertainty

Create a single measure of accuracy - above we used *compute_lowlowmetrics_all* to compute an accuracy measure based on

* relative error in annual minimum flow estimate
* relative error in monthly flow during low flow period
* correlation between observed and modelled annual minimum flow
* correlation between observed and modelled flow during the low flow period

We weighted all 4 the same

# Use the accuracy measure 

We can use the combined accuracy measure to define behavioural (acceptable) parameter set (**res_acc**) - two options

* define a threshold for acceptability (we will use 30%)
* take top 50 performing parameter sets

(we go with the latter but code could be commented to go with threshold approach)

# first step - define behavioral / acceptable parameter set
<a id="behavioral"></a>

```{r behavioral, echo=FALSE}

summary(res$combined)

# 1) selecting behaviorial or acceptable parameters sets

threshold = 0.3
res_acc = subset(res, combined > threshold)
head(res_acc)

# as an alternative  what if you want the top N parameter sets
topN = 50
tmp = res[order(res$combined,decreasing=T),]  # reorder from highest to lowest 
res_acc=tmp[1:topN,] # select the highest 50 
head(res_acc)

```

# Defining weights (likelihood) for parameter sets

Now define "weights" (likelihood) based on parameter performance for the acceptable or behaviorial parameters

We want the sum of the weights to equal 1

* accuracy measure defined above will define weight
* we divide by the sum of all accuracy measures to get fractions that add to 1
* note we now only work with behavioural parameter sets (in ** res_acc ** versus ** res **)

```{r weighting, echo=FALSE}

# create a weight for each parameter set based on its relative accuracy - we do this so all weights sum to 1
sum_acc=sum(res_acc$combined)
res_acc$wt_acc=res_acc$combined/sum_acc

head(res_acc)

# look at values
summary(res_acc$wt_acc)

# check to see that they sum to one
sum(res_acc$wt_acc)

Nacc = nrow(res_acc)
Nacc
```

# Using weights

One way to use weights is to define a maximum likelihood estimate by averaging (weighted by accuracy) streamflow from all behavioural simulations 



```{r mle, echo=FALSE}

# generate a streamflow as weighted average of all  acceptable parameter sets

# recall that msagel is the flow data for all runs so we 
# can link with weights from res_acc by run id
msagel  =  msage %>% pivot_longer(cols=!c(date, month, year, day,wy, obs), names_to="sim", values_to="flow")


# subset only acceptable runs
msagel_acc = subset(msagel, sim %in% res_acc$sim)
# join with weights from res_acc, left_join will repeat weights for each day in streamflow trajectory
msagel_acc = left_join(msagel_acc, res_acc, by="sim")
head(msagel_acc)
# finally multiply flow by weight
msagel_acc = msagel_acc %>% mutate(flow_wt = flow*wt_acc)

# now we can average streamflow for each day from all the runs # using the weights
aver_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(meanstr = sum(flow_wt))

# add some date information or simply add to simQ

ggplot(aver_flow, aes(x=date, y=meanstr))+geom_line(col="red")+labs(y="Streamflow mm/day")

# add some of the other date info and plot a subset
aver_flow$wy = msage$wy
wycheck=1985
ggplot(subset(aver_flow, wy == wycheck), aes(x=date, y=meanstr, col="model_wt"))+
  geom_line()+labs(y="Streamflow mm/day")+
  geom_line(data=subset(msage, wy==wycheck), aes(date, obs, col="obs")) 

```

# Final step of GLUE

We could also compute quantiles rather than just mean

We won't do this but just in case

We can use the `wtd.quantile()` function in the `reldist` package to do this - it computes quantiles accounting for different weights on each observation

```{r plotting, echo=TRUE}
 
# compute quantiles based on performance weights
quant_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(
  flow10=wtd.quantile(x=flow, weight=wt_acc, q=0.1),
  flow50=wtd.quantile(x=flow, weight=wt_acc, q=0.5),
  flow90=wtd.quantile(x=flow, weight=wt_acc, q=0.9)
)


# ad observed back
quant_flow_obs = left_join(quant_flow, msage[,c("date","month","year", "day","wy","obs")], 
                       by=c("date"))

# format for plotting
quant_flowl = quant_flow_obs %>% pivot_longer(col=c(flow10, flow50, flow90, obs), 
                                          values_to="flow", names_to="quantile")

# plot
ggplot(subset(quant_flowl, wy==1985), aes(date, flow, col=quantile))+geom_line()

# to see low flows, transform y-axis
ggplot(subset(quant_flowl, wy==1980), aes(date, flow, col=quantile))+
  geom_line()+scale_y_continuous(trans="log")


```

# Assignment

Final piece will be to produce a graph of maximum likelihood estimate given you acceptable parameters!

To hand in - an Rmarkdown and R function. Please knit and turn in either an html or pdf of the markdown. 

* Part 1 from above: R function that codes a metric for performance evaluation 
  * must be a combination of at least two performance measures
  * include some comments that explain 'why' this metric
  
* R markdown that does the following steps (with lots of documentation of the work flow):

  * Part 2 from above: 
    1. Apply your performance function to a subset of the Sagehen data set (with multiple simulations) that you want to use for calibration 
    2. Summarize the performance over the calibration period in 1-2 graphs; you can decide what is useful 
  
  * Part 3  
    3. Use the performance measure to select "acceptable" outcomes from parameter sets (see #15 in contents)
    4. Compute the range of the performance measure using only the "acceptable" outcomes over the post-calibration period (part that you didn't use for calibration in step 1)
    5. Graph the range of outcomes for acceptable parameters (e.g post-calibration parameter uncertainty); you can choose what output is most interesting for you 
    6. Compute and graph the maximum likelihood estimate of your output of interest (e.g minimum summer streamflow each year) for the post-calibration period (see #16 or #17 in contents)
  
  * Part 4: A short paragraph discussing why you choose the output and performance measures that you did and some thoughts (1-2 sentences) on what your calibration and post-calibration uncertainty analysis tells you
  
# Rubric 60 pts 

* R function (10pts) 
  * combines at least 2 performance metrics (5)
  * function is applied to part of Sagehen data set (5)
  
* Calibration (20pts)
  * your metrics are used to select 'acceptable' parameter set outcomes (5)
  * metrics are computed for post-calibration data of accepted parameter set outcomes (5)
  * maximum likelihood estimate is computed for post-calibration data (10)
  
* Graphs (20pts)
  * 1-2 plots of summary of performance over calibration period (5) 
  * 1-2 plots of output of acceptable parameter sets that clearly visualize uncertainty (5)
  * plot maximum likelihood estimate for post-calibration period (5) 
  * graphing style (axis labels, legibility) (5)
  
* Discussion (10pts)
  * short explanation on metrics used (5) 
  * 1-2 sentences on calibration and post-calibration uncertainty analysis 



